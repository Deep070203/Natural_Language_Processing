{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78892ce9",
   "metadata": {},
   "source": [
    "# Data Confidentiality in Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bf8427",
   "metadata": {},
   "source": [
    "Anonymizing sensitive information in text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f286c573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My name is [NAME].\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Example text\n",
    "text = \"My name is Deep Shah.\"\n",
    "\n",
    "# Simple pattern to match names\n",
    "name_pattern = r'\\b[A-Z][a-z]+ [A-Z][a-z]+\\b'\n",
    "\n",
    "# Replaces names with '[NAME]'\n",
    "anonymized_text = re.sub(name_pattern, '[NAME]', text)\n",
    "\n",
    "print(anonymized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97781145",
   "metadata": {},
   "source": [
    "Noise Removal in Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59697de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['beautiful', 'day']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/deepshah/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "text = \"It's a beautiful day!\"\n",
    "\n",
    "text_no_punc = re.sub(r'[^\\w\\s]','', text)\n",
    "\n",
    "tokens = text_no_punc.split()\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "filtered_text = [word for word in tokens if not word.lower() in stop_words]\n",
    "\n",
    "print(filtered_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e9a583",
   "metadata": {},
   "source": [
    "## Analyzing a Text Dataset Addressing Missing Data and Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95dfe6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58b476c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID Social Media Activity Questionnaire Completions Product Feedback  \\\n",
      "0   1                Active                 Completed         Positive   \n",
      "1   2              Inactive             Not Completed          Neutral   \n",
      "2   3                Active                 Completed         Negative   \n",
      "3   4                Active                 Completed         Positive   \n",
      "4   5              Inactive             Not Completed         Positive   \n",
      "\n",
      "          Notes From Previous Conversations  \n",
      "0                Interested in new features  \n",
      "1               Asked about pricing options  \n",
      "2        Expressed concerns about usability  \n",
      "3            Recommended product to friends  \n",
      "4  Asked for additional product information  \n"
     ]
    }
   ],
   "source": [
    "# Load Dataset\n",
    "file_path = 'Qualitative data.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42568278",
   "metadata": {},
   "source": [
    "### Identify Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a00a504e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID                                   0\n",
      "Social Media Activity                0\n",
      "Questionnaire Completions            0\n",
      "Product Feedback                     0\n",
      "Notes From Previous Conversations    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d347b6b",
   "metadata": {},
   "source": [
    "### Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e13b5730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling missing data (fill with a placeholder)\n",
    "data.fillna('Missing', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac58f1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling missing data (drop missing value)\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50f4640",
   "metadata": {},
   "source": [
    "### Identify and Clean Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f53e320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean noise from text\n",
    "def clean_noise(text):\n",
    "    # remove special characters, number, punctuations except for emojis\n",
    "    text = re.sub(r'[^a-zA-Z\\s\\U0001F600-\\U0001F64F]', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0c7b104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                  Interested in new features\n",
      "1                 Asked about pricing options\n",
      "2          Expressed concerns about usability\n",
      "3              Recommended product to friends\n",
      "4    Asked for additional product information\n",
      "Name: Cleaned_Text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "data['Cleaned_Text'] = data['Notes From Previous Conversations'].apply(clean_noise)\n",
    "\n",
    "print(data['Cleaned_Text'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5312e0cc",
   "metadata": {},
   "source": [
    "## Advanced Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc56b93",
   "metadata": {},
   "source": [
    "### The Lemmatization Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "300587b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The runner were sprinting during the final few mile of the competition\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sentence = \"The runners were sprinting during the final few miles of the competition\"\n",
    "\n",
    "lemmatized = ' '.join([lemmatizer.lemmatize(w) for w in nltk.word_tokenize(sentence)])\n",
    "\n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071f5bcc",
   "metadata": {},
   "source": [
    "### Various Tokenization Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34da4392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'goalkeeper'), ('goalkeeper', 'made'), ('made', 'a'), ('a', 'great'), ('great', 'save'), ('save', 'during'), ('during', 'the'), ('the', 'game')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "\n",
    "sentence = \"The goalkeeper made a great save during the game\"\n",
    "\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "biggrams = list(ngrams(tokens, 2))\n",
    "\n",
    "print(biggrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1caa047",
   "metadata": {},
   "source": [
    "### Language Detection and Handling Specialized Toeknizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "147f9b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Il', 'ne', 'faut', 'rien', 'laisser', 'au', 'hasard', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/deepshah/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# !pip install langdetect\n",
    "from langdetect import detect\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sentence = \"Il ne faut rien laisser au hasard.\"\n",
    "\n",
    "lang = detect(sentence)\n",
    "\n",
    "if lang == \"fr\":\n",
    "    nltk.download('punkt')\n",
    "    tokens = word_tokenize(sentence, language=\"french\")\n",
    "    print(f\"Tokens: {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff704141",
   "metadata": {},
   "source": [
    "### Normalization and Tokenization Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "91280618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting emoji\n",
      "  Downloading emoji-2.12.1-py3-none-any.whl (431 kB)\n",
      "\u001b[K     |████████████████████████████████| 431 kB 840 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.7.0 in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (from emoji) (4.12.1)\n",
      "Installing collected packages: emoji\n",
      "Successfully installed emoji-2.12.1\n"
     ]
    }
   ],
   "source": [
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49c96845",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/deepshah/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b3e6aa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "file_path = 'Qualitative data.csv'\n",
    "data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "89f69afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize text and handle emojis\n",
    "def normalize_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Convert emojis to text\n",
    "    text = emoji.demojize(text, delimiters=(\" \", \" \"))\n",
    "\n",
    "    # Remove special characters (keeping spaces)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "data['Normalized_Text'] = data['Notes From Previous Conversations'].apply(normalize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1cf397bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "# data['Tokens'] = data['Normalized_Text'].str.split()\n",
    "\n",
    "# Using NLTK\n",
    "data['Tokens'] = data['Normalized_Text'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3778a147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Normalized_Text</th>\n",
       "      <th>Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>interested in new features</td>\n",
       "      <td>[interested, in, new, features]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>asked about pricing options</td>\n",
       "      <td>[asked, about, pricing, options]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>expressed concerns about usability</td>\n",
       "      <td>[expressed, concerns, about, usability]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>recommended product to friends</td>\n",
       "      <td>[recommended, product, to, friends]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>asked for additional product information</td>\n",
       "      <td>[asked, for, additional, product, information]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Normalized_Text  \\\n",
       "0                interested in new features   \n",
       "1               asked about pricing options   \n",
       "2        expressed concerns about usability   \n",
       "3            recommended product to friends   \n",
       "4  asked for additional product information   \n",
       "\n",
       "                                           Tokens  \n",
       "0                 [interested, in, new, features]  \n",
       "1                [asked, about, pricing, options]  \n",
       "2         [expressed, concerns, about, usability]  \n",
       "3             [recommended, product, to, friends]  \n",
       "4  [asked, for, additional, product, information]  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[['Normalized_Text', 'Tokens']].head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5511d92f",
   "metadata": {},
   "source": [
    "# 1. Text Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33da51cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (3.7)\n",
      "Requirement already satisfied: click in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (from nltk) (8.1.4)\n",
      "Requirement already satisfied: joblib in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: tqdm in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (from nltk) (2022.3.15)\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.7.4-cp39-cp39-macosx_10_9_x86_64.whl (6.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.9 MB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.10-cp39-cp39-macosx_10_9_x86_64.whl (26 kB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4\n",
      "  Downloading pydantic-2.7.3-py3-none-any.whl (409 kB)\n",
      "\u001b[K     |████████████████████████████████| 409 kB 8.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.4.0-py3-none-any.whl (182 kB)\n",
      "\u001b[K     |████████████████████████████████| 182 kB 2.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.8-cp39-cp39-macosx_10_9_x86_64.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.8-cp39-cp39-macosx_10_9_x86_64.whl (493 kB)\n",
      "\u001b[K     |████████████████████████████████| 493 kB 8.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting thinc<8.3.0,>=8.2.2\n",
      "  Downloading thinc-8.2.3-cp39-cp39-macosx_10_9_x86_64.whl (880 kB)\n",
      "\u001b[K     |████████████████████████████████| 880 kB 5.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting smart-open<7.0.0,>=5.2.1\n",
      "  Downloading smart_open-6.4.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 6.9 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (from spacy) (61.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: jinja2 in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (from spacy) (3.1.2)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.9-cp39-cp39-macosx_10_9_x86_64.whl (133 kB)\n",
      "\u001b[K     |████████████████████████████████| 133 kB 8.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.19.0 in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (from spacy) (1.22.4)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting typer<0.10.0,>=0.3.0\n",
      "  Downloading typer-0.9.4-py3-none-any.whl (45 kB)\n",
      "\u001b[K     |████████████████████████████████| 45 kB 3.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting weasel<0.4.0,>=0.1.0\n",
      "  Downloading weasel-0.3.4-py3-none-any.whl (50 kB)\n",
      "\u001b[K     |████████████████████████████████| 50 kB 8.1 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting wasabi<1.2.0,>=0.9.1\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (from spacy) (4.64.0)\n",
      "Collecting language-data>=1.2\n",
      "  Downloading language_data-1.2.0-py3-none-any.whl (5.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.4 MB 7.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting marisa-trie>=0.7.7\n",
      "  Downloading marisa_trie-1.1.1-cp39-cp39-macosx_10_9_x86_64.whl (193 kB)\n",
      "\u001b[K     |████████████████████████████████| 193 kB 9.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->spacy) (3.0.4)\n",
      "Collecting annotated-types>=0.4.0\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting typing-extensions>=4.6.1\n",
      "  Downloading typing_extensions-4.12.1-py3-none-any.whl (37 kB)\n",
      "Collecting pydantic-core==2.18.4\n",
      "  Downloading pydantic_core-2.18.4-cp39-cp39-macosx_10_12_x86_64.whl (1.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.9 MB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: idna<4,>=2.5 in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.9)\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.11-cp39-cp39-macosx_10_9_x86_64.whl (6.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.1 MB 3.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: click<9.0.0,>=7.1.1 in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.4)\n",
      "Collecting cloudpathlib<0.17.0,>=0.7.0\n",
      "  Downloading cloudpathlib-0.16.0-py3-none-any.whl (45 kB)\n",
      "\u001b[K     |████████████████████████████████| 45 kB 4.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (from jinja2->spacy) (2.1.3)\n",
      "Installing collected packages: typing-extensions, pydantic-core, catalogue, annotated-types, srsly, pydantic, murmurhash, marisa-trie, cymem, wasabi, typer, smart-open, preshed, language-data, confection, cloudpathlib, blis, weasel, thinc, spacy-loggers, spacy-legacy, langcodes, spacy\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 4.1.1\n",
      "    Uninstalling typing-extensions-4.1.1:\n",
      "      Successfully uninstalled typing-extensions-4.1.1\n",
      "  Attempting uninstall: smart-open\n",
      "    Found existing installation: smart-open 5.1.0\n",
      "    Uninstalling smart-open-5.1.0:\n",
      "      Successfully uninstalled smart-open-5.1.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.12.1 which is incompatible.\u001b[0m\n",
      "Successfully installed annotated-types-0.7.0 blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.16.0 confection-0.1.5 cymem-2.0.8 langcodes-3.4.0 language-data-1.2.0 marisa-trie-1.1.1 murmurhash-1.0.10 preshed-3.0.9 pydantic-2.7.3 pydantic-core-2.18.4 smart-open-6.4.0 spacy-3.7.4 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.3 typer-0.9.4 typing-extensions-4.12.1 wasabi-1.1.3 weasel-0.3.4\n"
     ]
    }
   ],
   "source": [
    "# Install libraries\n",
    "!pip install nltk\n",
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0a99a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the library\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68dff40b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ben relocated to Paris last year to pursue his passion. His currently enrolles in a comprehensive course on Natural Language Processing'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Ben relocated to Paris last year to pursue his passion. His currently enrolles in a comprehensive course on Natural Language Processing\"\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf73887e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/deepshah/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "515efb88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ben', 'relocated', 'to', 'Paris', 'last', 'year', 'to', 'pursue', 'his', 'passion', '.', 'His', 'currently', 'enrolles', 'in', 'a', 'comprehensive', 'course', 'on', 'Natural', 'Language', 'Processing']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a929f1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ben', 'reloc', 'to', 'pari', 'last', 'year', 'to', 'pursu', 'hi', 'passion', '.', 'hi', 'current', 'enrol', 'in', 'a', 'comprehens', 'cours', 'on', 'natur', 'languag', 'process']\n"
     ]
    }
   ],
   "source": [
    "# Stemming = reduces words to their root form\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_words = [stemmer.stem(token) for token in tokens]\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "691de6dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/deepshah/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0386f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Ben', 'NNP'), ('relocated', 'VBD'), ('to', 'TO'), ('Paris', 'NNP'), ('last', 'JJ'), ('year', 'NN'), ('to', 'TO'), ('pursue', 'VB'), ('his', 'PRP$'), ('passion', 'NN'), ('.', '.'), ('His', 'PRP$'), ('currently', 'RB'), ('enrolles', 'VBZ'), ('in', 'IN'), ('a', 'DT'), ('comprehensive', 'JJ'), ('course', 'NN'), ('on', 'IN'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "# Part-of-speech tagging\n",
    "from nltk import pos_tag\n",
    "text_pos_tag = pos_tag(tokens)\n",
    "print(text_pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f84790bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     /Users/deepshah/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping help/tagsets.zip.\n"
     ]
    }
   ],
   "source": [
    "# This function shows us the label for each tag\n",
    "nltk.download('tagsets')\n",
    "nltk.help.upenn_tagset(\"NNP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "00abf994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Ben/NNP)\n",
      "  relocated/VBD\n",
      "  to/TO\n",
      "  (GPE Paris/NNP)\n",
      "  last/JJ\n",
      "  year/NN\n",
      "  to/TO\n",
      "  pursue/VB\n",
      "  his/PRP$\n",
      "  passion/NN\n",
      "  ./.\n",
      "  His/PRP$\n",
      "  currently/RB\n",
      "  enrolles/VBZ\n",
      "  in/IN\n",
      "  a/DT\n",
      "  comprehensive/JJ\n",
      "  course/NN\n",
      "  on/IN\n",
      "  (ORGANIZATION Natural/NNP Language/NNP)\n",
      "  Processing/NNP)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/deepshah/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/deepshah/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Named Entitiy Recognition\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "entities = nltk.ne_chunk(text_pos_tag)\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a9b64f",
   "metadata": {},
   "source": [
    "# 2. Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c3e150cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-lg==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.7.1/en_core_web_lg-3.7.1-py3-none-any.whl (587.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 587.7 MB 18 kB/s  eta 0:00:012     |██████████████████████▎         | 409.7 MB 4.1 MB/s eta 0:00:44     |████████████████████████▎       | 446.0 MB 11.3 MB/s eta 0:00:13\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (from en-core-web-lg==3.7.1) (3.7.4)\n",
      "Requirement already satisfied: jinja2 in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.1.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.64.0)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.1.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.4.0)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.9.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (21.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: setuptools in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (61.2.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.7.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.2.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.22.4)\n",
      "Requirement already satisfied: language-data>=1.2 in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.12.1)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.18.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2021.10.8)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.1.5)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.1.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.1.3)\n",
      "Installing collected packages: en-core-web-lg\n",
      "Successfully installed en-core-web-lg-3.7.1\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "89915653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6108841234425123\n",
      "0.19521993793686707\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English model\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# Process words with the model\n",
    "word1 = nlp(\"king\")\n",
    "word2 = nlp(\"queen\")\n",
    "word3 = nlp(\"apple\")\n",
    "\n",
    "# Calculate the similarities\n",
    "similarity1 = word1.similarity(word2)\n",
    "similarity2 = word1.similarity(word3)\n",
    "\n",
    "# Display the similarities\n",
    "print(similarity1)\n",
    "print(similarity2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691197eb",
   "metadata": {},
   "source": [
    "# 3. NLP Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c4e07c",
   "metadata": {},
   "source": [
    "## Data Acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd49820",
   "metadata": {},
   "source": [
    "Sourcing various texts from digital places digital libraries, websites, transcripts\n",
    "\n",
    "Public data, Web Scraping, APIs\n",
    "\n",
    "Data Augmentation - Enhancing our dataset by making slight variations to existing texts (Synonyms, Rephrasing, increasing volume)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d060e676",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "posts = fetch_20newsgroups(subset='all', categories=['sci.electronics', 'sci.space'],\n",
    "                          remove = ('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fbc9ef53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\n"
     ]
    }
   ],
   "source": [
    "# See the available key in the dataset\n",
    "print(posts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "79b44768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL>>        Question:   Is there a certain device out there that I can\n",
      "AL>>                    use to find out the number to the line?\n",
      "AL>>        Thanks for any response.\n",
      "AL>>                                                    Al\n",
      "\n",
      "AL>There is a number you can call which will return a synthesized\n",
      "AL>voice telling you the number of the line.  Unfortunately, for the\n",
      "AL>life of me I can't remember what it is. The telephone technicians\n",
      "AL>use it all the time.  We used to play around with this in our\n",
      "AL>dorm rooms since there were multiple phone lines running between\n",
      "AL>rooms.\n",
      "\n",
      "It probably wouldn't help for you to post the number, since it appears\n",
      "to be different in each area.  For what it's worth, in the New Orleans\n",
      "area the number is 998-877-6655 (easy to remember, what?)\n",
      "\n",
      "\n",
      " * SLMR 2.1 * Ask me anything: if I don't know, I'll make up something.\n",
      "                                          \n"
     ]
    }
   ],
   "source": [
    "# Display one post\n",
    "print(posts.data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7e187100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text            label\n",
      "0  \\n   >\\tIf the  new  Kuiper belt object *is*  ...        sci.space\n",
      "1  AL>>        Question:   Is there a certain dev...  sci.electronics\n",
      "2  \\nIt's not quite what you were asking, but a f...        sci.space\n",
      "3  \\n\\n\\nNo, the sky does not, at this time, belo...        sci.space\n",
      "4   \\nDigi-Key also sells Quad Line Receivers, pa...  sci.electronics\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1971, 2)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'text': posts.data,\n",
    "    'label': [posts.target_names[target] for target in posts.target]\n",
    "})\n",
    "\n",
    "print(df.head())\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2ffde5",
   "metadata": {},
   "source": [
    "## Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bff6344",
   "metadata": {},
   "source": [
    "We ensure that data is validated and filtered to guarantee accuracy, relevance and the absence of errors or duplicates.\n",
    "\n",
    "Step 1: Clean and Standardize the Data \n",
    "\n",
    "Addressing issues like Spelling, grammar, typographical errors, and whitespaces\n",
    "\n",
    "Standardizing capitalization, date formats, encoding\n",
    "\n",
    "Discarding irrelevant elements like metadata or non-textual content\n",
    "\n",
    "Step 2: Deconstruct Natural Language\n",
    "\n",
    "Tokenization -> Remove Stop Words (like and) -> Stemming (Faster but less precise) and Lemmatization (more accurate and needs more resource) to reduce words to their base forms -> POS tagging (categoriing each token according to grammatical role such as noun or verb) and Named Entity Recognition (Identifies or Classifies key information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f801e58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/deepshah/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/deepshah/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/deepshah/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d9bcf9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to clean and pre-process text\n",
    "def clean_text(text):\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove tokens that are not purely letters\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    \n",
    "    # Lowercase the text\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    # Join tokens back into a single string\n",
    "    clean_text = ' '.join(tokens)\n",
    "    \n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e7b4141a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text            label  \\\n",
      "0  \\n   >\\tIf the  new  Kuiper belt object *is*  ...        sci.space   \n",
      "1  AL>>        Question:   Is there a certain dev...  sci.electronics   \n",
      "2  \\nIt's not quite what you were asking, but a f...        sci.space   \n",
      "3  \\n\\n\\nNo, the sky does not, at this time, belo...        sci.space   \n",
      "4   \\nDigi-Key also sells Quad Line Receivers, pa...  sci.electronics   \n",
      "\n",
      "                                          clean_text  \n",
      "0  new kuiper belt object called next one called ...  \n",
      "1  al question certain device al use find number ...  \n",
      "2  quite asking year ago helped ee remote sensing...  \n",
      "3  sky time belong anyone ownership necessary def...  \n",
      "4  also sell quad line receiver part quad line dr...  \n"
     ]
    }
   ],
   "source": [
    "# Apply the function to the data\n",
    "df['clean_text'] = df['text'].apply(clean_text)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2521342c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          clean_text            label\n",
      "0  new kuiper belt object called next one called ...        sci.space\n",
      "1  al question certain device al use find number ...  sci.electronics\n",
      "2  quite asking year ago helped ee remote sensing...        sci.space\n",
      "3  sky time belong anyone ownership necessary def...        sci.space\n",
      "4  also sell quad line receiver part quad line dr...  sci.electronics\n"
     ]
    }
   ],
   "source": [
    "# Keep only clean text column and label\n",
    "clean_data = df[['clean_text', 'label']]\n",
    "print(clean_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64061066",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50d8384",
   "metadata": {},
   "source": [
    "Convert Preprocessed texts into numerical format or NLP algos.\n",
    "\n",
    "One-hot encoding: Each word is encoded as a unique vector represented as one in its position against the entire vocab which is represented as zeroes. This creates a sparse matrix which is computationally inefficient.\n",
    "\n",
    "Bag-of-Words Model counters sparsity by counting word frequencies without considering grammar or word order useful for classifying texts with similar words.\n",
    "\n",
    "The more frequent the word, the more important it might be to the text. Drawbacks - Ignores word order.\n",
    "\n",
    "Bag-of-n-grams Model extends this by considering a sequence of n words together capturing more context. Increasing n provides richer detail but can lead to feature explosion risking model overfitting.\n",
    "\n",
    "TF-IDF (Term Frequency Inverse Document Frequency) helps identifying the most significant words in a document. TF counts how often a words appears in a given document, while IDF measures the importance of a term across a corpus. Mathematically, TF-IDF is their product, assigning a score that reflects the word's importance in a document relative to a collection of documents. Higher TF-IDF score suggests a great relevance. IDF suggests rarity.\n",
    "\n",
    "Word Embedding are another type of representation in NLP where words froma vocab are mapped to vectors of real numbers in a predefined space. Each word is represented by a dense vector that capture its semantic meaning based on the context in which it appears. These embeddings are learned from large corpora of text data. Popular Model: Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bc962f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1478,) (1478,)\n",
      "(493,) (493,)\n"
     ]
    }
   ],
   "source": [
    "# Perform the train-test split\n",
    "X = df['clean_text']\n",
    "y = df['label']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c220c43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ability  able  absolutely  ac  acceleration  accept  accepted  access  \\\n",
      "0        0     0           0   0             0       0         0       0   \n",
      "1        0     0           0   0             0       0         0       0   \n",
      "2        0     0           0   0             0       0         0       0   \n",
      "3        0     0           0   0             0       0         0       0   \n",
      "4        0     0           0   0             0       0         0       0   \n",
      "\n",
      "   accomplish  according  ...  wrong  wrote  yeah  year  yellow  yes  \\\n",
      "0           0          0  ...      0      0     0     0       0    0   \n",
      "1           0          0  ...      0      0     0     0       0    0   \n",
      "2           0          0  ...      0      0     0     0       0    0   \n",
      "3           0          0  ...      0      0     0     0       0    0   \n",
      "4           0          0  ...      0      0     0     0       0    0   \n",
      "\n",
      "   yesterday  yet  york  zero  \n",
      "0          0    0     0     0  \n",
      "1          0    0     0     0  \n",
      "2          0    0     0     0  \n",
      "3          0    0     0     0  \n",
      "4          0    0     0     0  \n",
      "\n",
      "[5 rows x 1780 columns]\n"
     ]
    }
   ],
   "source": [
    "# Vectorize the data\n",
    "\n",
    "# Bag-of-Words\n",
    "from sklearn.feature_extraction.text import CountVectorizer # counts how many times each word appears in a vocab\n",
    "count_vect = CountVectorizer(min_df=10) # , ngram_range = (2, 2)) # 2 word combinations. Sets the lower and upper boundary of the range of n values for different word n grams.\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_counts = count_vect.fit_transform(X_train) # the method fits the model or learns the vocab of the training set and then it transforms the training data into a matrix of token counts\n",
    "\n",
    "# Transform the test data\n",
    "X_test_counts = count_vect.transform(X_test) # Only transform do not fit. This means that the test data will be transformed into a matrix of token counts using the vocab learn from the training data preventing data leakage\n",
    "\n",
    "# Display feature names\n",
    "counts_df = pd.DataFrame(X_train_counts.toarray(), columns = count_vect.get_feature_names_out())\n",
    "print(counts_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "51555873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   able   ac  acceleration  access  according  across  act  action  active  \\\n",
      "0   0.0  0.0           0.0     0.0        0.0     0.0  0.0     0.0     0.0   \n",
      "1   0.0  0.0           0.0     0.0        0.0     0.0  0.0     0.0     0.0   \n",
      "2   0.0  0.0           0.0     0.0        0.0     0.0  0.0     0.0     0.0   \n",
      "3   0.0  0.0           0.0     0.0        0.0     0.0  0.0     0.0     0.0   \n",
      "4   0.0  0.0           0.0     0.0        0.0     0.0  0.0     0.0     0.0   \n",
      "\n",
      "   activity  ...     would  write  writing  written  wrong  wrote  year  yes  \\\n",
      "0       0.0  ...  0.070156    0.0      0.0      0.0    0.0    0.0   0.0  0.0   \n",
      "1       0.0  ...  0.000000    0.0      0.0      0.0    0.0    0.0   0.0  0.0   \n",
      "2       0.0  ...  0.000000    0.0      0.0      0.0    0.0    0.0   0.0  0.0   \n",
      "3       0.0  ...  0.000000    0.0      0.0      0.0    0.0    0.0   0.0  0.0   \n",
      "4       0.0  ...  0.000000    0.0      0.0      0.0    0.0    0.0   0.0  0.0   \n",
      "\n",
      "   yet  zero  \n",
      "0  0.0   0.0  \n",
      "1  0.0   0.0  \n",
      "2  0.0   0.0  \n",
      "3  0.0   0.0  \n",
      "4  0.0   0.0  \n",
      "\n",
      "[5 rows x 1210 columns]\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vect = TfidfVectorizer(max_df=0.7, min_df=0.01) # Restricting the minimum and maximum document frequency\n",
    "\n",
    "# Fit and transform the training data\n",
    "tfidf_train = tfidf_vect.fit_transform(X_train) \n",
    "\n",
    "# Transform the test data\n",
    "tfidf_test = tfidf_vect.transform(X_test)\n",
    "\n",
    "# Display feature names\n",
    "tfidf_df = pd.DataFrame(tfidf_train.toarray(), columns = tfidf_vect.get_feature_names_out())\n",
    "print(tfidf_df.head()) # this helps for preparing data for using in a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e65d35",
   "metadata": {},
   "source": [
    "## Data Modeling and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892beb2a",
   "metadata": {},
   "source": [
    "Models:\n",
    "    \n",
    "    Text Classification (Classifying Emails as Spam or Organizing Articles) using Naive Bayes, Support Vector Machines (SVM) and Neural Networks\n",
    "    \n",
    "    Sentiment Analysis (Discerns emotions or sentiments with text for Brand Monitoring and Analysing customer feedback) using Logistic Regression, Long Short Term Memory Networks (LSTMs)\n",
    "    \n",
    "    Named Entity Recognition is about identifying and categorizing key elements in text such as names of people, places etc. using Conditional Random Fields and transformer based models like BERT\n",
    "    \n",
    "    Speech Recognition is conversion of spoken lang into text using Hidden Markov Models and now emplying deep neural networks like LSTMs and RNN\n",
    "    \n",
    "    Text generation with LSTMs and GPT\n",
    "    \n",
    "    Machine Translation using Statistical Machine translation to more advanced models such as Seq2Seq, GPT\n",
    "\n",
    "\n",
    "Evaluate our Model's Performance:\n",
    "    \n",
    "    Use various metrics including accuracy (proportion of correct prediction), precision (looks at proportion of two positive predictions in the total positive predictions), recall (Assesses the proportion of actual positive correctly identified), and F-score\n",
    "    \n",
    "    Adjusting parameters, choosing different algos or revising data cleaning methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a4b67669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9006085192697769"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use Naive Bayes classifier since it is particularly suited for count vectorized data as it expects integer inputs.\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_counts, y_train)\n",
    "y_pred = nb.predict(X_test_counts)\n",
    "metrics.accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "804c5f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "                 sci.electronics  sci.space\n",
      "sci.electronics              227          7\n",
      "sci.space                     42        217\n"
     ]
    }
   ],
   "source": [
    "labels = ['sci.electronics', 'sci.space']\n",
    "cm = metrics.confusion_matrix(y_test, y_pred, labels=labels)\n",
    "\n",
    "# Create a DataFrame from the Confusion Matrix \n",
    "cm_df = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "\n",
    "# Print\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f829c022",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f19562",
   "metadata": {},
   "source": [
    "It is the unsupervised classification of text documents to find \"topics\".\n",
    "\n",
    "Since we don't know what topics are within the text, the topics are known as hidden or \"latent\".\n",
    "\n",
    "Key Foundations:\n",
    "\n",
    "    1. Documents are mixtures of topics\n",
    "    2. Topics are mixtures of words\n",
    "\n",
    "A document is represented as a distribution oer topics and a topic is a distribution over words (Document-topic and topic-word distributions)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a9412e",
   "metadata": {},
   "source": [
    "Topic Modeling Algorithms:\n",
    "\n",
    "    1. Latent Dirichlet Allocation (LDA): \n",
    "        A generative statistical model using Bayesian techniques.\n",
    "        Views documents as bag of words (no relations/order)\n",
    "    2. Non-negative Matrix Factorization (NMF): \n",
    "        Finds document-topic and term-topic matrices by minimizing a cost function\n",
    "    3. BertTopic:\n",
    "        Use transformer-based methods\n",
    "        Build embedding, compress + cluster topics, extract topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763aefba",
   "metadata": {},
   "source": [
    "## Latent Dirchlet Allocation (LDA)\n",
    "\n",
    "Mechanics:\n",
    "\n",
    "LDA is a generative method to fit a topic model - it generates words using a probabilistic model.\n",
    "\n",
    "LDA treats each document as a mixture of topics, and each topic as a bag of words, allowing documents to overlap in content.\n",
    "\n",
    "For each document in a corpus:\n",
    "\n",
    "    Choose N ~ Poisson(ξ) (Determine the document length acc to a Poisson distribution)\n",
    "    \n",
    "    Choose θ ~ Dir(α)     (Choose a topic mixture acc to a Dirichlet distribution (over a fixed set of topics))\n",
    "\n",
    "    For each of the N words:\n",
    "    \n",
    "        (a) Choose a topic zn ~ Multinomial(θ) (For each word, sample a topic from the multinomial distribution\n",
    "        \n",
    "        (b) Choose a word wn from p(wn|zn, β), a multinomial probability conditioned on the topic zn. \n",
    "        \n",
    "\n",
    "Evaluation Metrics:\n",
    "\n",
    "    Perplexity: Tells us how well the model describes a set of documents, Low is better\n",
    "    \n",
    "    Topic Coeherence: Measures how similar the top words are to each other in a single topic, Higher is better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e119b394",
   "metadata": {},
   "source": [
    "### Implement LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922d251c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8ece814",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/deepshah/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb33beba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyLDAvis\n",
      "  Downloading pyLDAvis-3.4.1-py3-none-any.whl (2.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.6 MB 1.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (from pyLDAvis) (61.2.0)\n",
      "Requirement already satisfied: jinja2 in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (from pyLDAvis) (3.1.2)\n",
      "Collecting joblib>=1.2.0\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[K     |████████████████████████████████| 301 kB 10.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (from pyLDAvis) (1.7.3)\n",
      "Collecting funcy\n",
      "  Downloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n",
      "Requirement already satisfied: gensim in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (from pyLDAvis) (4.1.2)\n",
      "Collecting numpy>=1.24.2\n",
      "  Downloading numpy-1.26.4-cp39-cp39-macosx_10_9_x86_64.whl (20.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 20.6 MB 9.6 MB/s eta 0:00:01    |████▋                           | 3.0 MB 1.2 MB/s eta 0:00:15\n",
      "\u001b[?25hCollecting pandas>=2.0.0\n",
      "  Downloading pandas-2.2.2-cp39-cp39-macosx_10_9_x86_64.whl (12.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.6 MB 11.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numexpr in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (from pyLDAvis) (2.8.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (from pyLDAvis) (1.0.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (from pandas>=2.0.0->pyLDAvis) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (from pandas>=2.0.0->pyLDAvis) (2.8.2)\n",
      "Collecting tzdata>=2022.7\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "\u001b[K     |████████████████████████████████| 345 kB 9.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn>=1.0.0->pyLDAvis) (2.2.0)\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.13.1-cp39-cp39-macosx_10_9_x86_64.whl (39.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 39.4 MB 935 kB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (from gensim->pyLDAvis) (6.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (from jinja2->pyLDAvis) (2.1.3)\n",
      "Requirement already satisfied: packaging in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (from numexpr->pyLDAvis) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (from packaging->numexpr->pyLDAvis) (3.0.4)\n",
      "Installing collected packages: numpy, tzdata, scipy, joblib, pandas, funcy, pyLDAvis\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.22.4\n",
      "    Uninstalling numpy-1.22.4:\n",
      "      Successfully uninstalled numpy-1.22.4\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.7.3\n",
      "    Uninstalling scipy-1.7.3:\n",
      "      Successfully uninstalled scipy-1.7.3\n",
      "  Attempting uninstall: joblib\n",
      "    Found existing installation: joblib 1.1.0\n",
      "    Uninstalling joblib-1.1.0:\n",
      "      Successfully uninstalled joblib-1.1.0\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.4.2\n",
      "    Uninstalling pandas-1.4.2:\n",
      "      Successfully uninstalled pandas-1.4.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "daal4py 2021.5.0 requires daal==2021.4.0, which is not installed.\n",
      "tensorflow 2.13.0 requires numpy<=1.24.3,>=1.22, but you have numpy 1.26.4 which is incompatible.\n",
      "tensorflow 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.12.1 which is incompatible.\n",
      "numba 0.55.1 requires numpy<1.22,>=1.18, but you have numpy 1.26.4 which is incompatible.\u001b[0m\n",
      "Successfully installed funcy-2.0 joblib-1.4.2 numpy-1.26.4 pandas-2.2.2 pyLDAvis-3.4.1 scipy-1.13.1 tzdata-2024.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bddf36ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (4.1.2)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/deepshah/opt/anaconda3/lib/python3.9/site-packages (from gensim) (6.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39cee9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up 20 Newsgroups data\n",
    "from string import punctuation\n",
    "from nltk import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Obtain the dataset\n",
    "newsgroups = fetch_20newsgroups()\n",
    "\n",
    "# Define our stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Use RegexpTokenizer to split string into substrings using a reg ex.\n",
    "tokenizer = RegexpTokenizer(r'\\s+', gaps=True)\n",
    "\n",
    "# Stem the words\n",
    "stemmer = PorterStemmer()\n",
    "translate_tab = {ord(p): u\" \" for p in punctuation}\n",
    "\n",
    "def text2tokens(raw_text):\n",
    "    \"\"\"Split the raw_text string into a list of stemmed tokens\"\"\"\n",
    "    # With Python .translate, some chars are replaced with the char desc in a dict\n",
    "    clean_text = raw_text.lower().translate(translate_tab)\n",
    "    \n",
    "    # Apply tokenizer\n",
    "    tokens = [token.strip() for token in tokenizer.tokenize(clean_text)]\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Apply Stemmer\n",
    "    stemmed = [stemmer.stem(token) for token in tokens]\n",
    "    return [token for token in stemmed if len(token) > 2] # skip short ones\n",
    "\n",
    "# Convert a document to a list of tokens\n",
    "data = [text2tokens(text) for text in newsgroups['data']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474f0134",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# A Dictionary is mapping between words and their int ids\n",
    "dictionary = Dictionary(documents=data, prune_at=None)\n",
    "\n",
    "# Use Dictionary to remove un-relevant tokens\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.3, keep_n=None)\n",
    "\n",
    "# Assign new word IDs to all words, to make IDs more compact after filtering\n",
    "dictionary.compactify()\n",
    "\n",
    "# Convert the list of tokens to the bag of word representation\n",
    "bow_dataset = [dictionary.doc2bow(doc) for doc in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1a49ad",
   "metadata": {},
   "source": [
    "#### Train(fit) the LDA Model\n",
    "Key parameters and hyperparameters:\n",
    "\n",
    "    1. eval_every: The log perplexity score is estimated every that many updates. Setting this one slows down training by ~2x, so we set it to None.\n",
    "    2. passes: Number of passes thorugh the corpus during training.\n",
    "    3. workers: Number of worker processes to be used for parallelization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bd419d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaMulticore\n",
    "# Define the number of topics - we'll cover how to determine this later\n",
    "num_topics = 15\n",
    "\n",
    "# LdaMulticore is an istantiation of an online LDA algo. It uses all cpu cores to parallelize and speed up model training\n",
    "lda1 = LdaMulticore(\n",
    "        corpus=bow_dataset, num_topics=num_topics, id2word=dictionary,\n",
    "        workers=4, eval_every=None, passes=10, batch=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e686e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda2 = LdaMulticore(\n",
    "        corpus=bow_dataset, num_topics=num_topics, id2word=dictionary,\n",
    "        workers=4, eval_every=None, passes=20,\n",
    "        # alpha represents an A-priori belief on document-topic distribution; default is 1.0/num_topics\n",
    "        alpha=(5.0/num_topics),\n",
    "        # eta represents an A-priori belief on document-topic distribution; default is 1.0/num_topics\n",
    "        eta=0.01,\n",
    "        batch=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1813669b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the results of LDA\n",
    "import pyLDAvis.gensim_models\n",
    "import pyLDAvis\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "LDAvis_prepared = pyLDAvis.gensim_models.prepare(lda_fst, d2b_dataset, dictionary)\n",
    "LDAvis_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f69d0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Coherence Score\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Compute Coherence score using C_V\n",
    "coherence_model = CoherenceModel(model=lda1, texts=data, dictionary=dictionary, coherence='c_v') \n",
    "# c_v score is measured based on a sliding window and it leverages normalized poitnwise mutual information and cosine similarity\n",
    "coherence = coherence_model.get_coherence()\n",
    "print(\"\\nCoherence Score: \", coherence) # score = 0.50\n",
    "# Do the same for second model score: 0.5649"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72c8f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating model perplexity\n",
    "# Using the .log_perplexity method, we can calculate and return per-word liklihood bound, using the chunk of documents as evaluation corpus.\n",
    "# This method doesn't return perplexity score but returns the \"per-word-bound\": -ve of the log of perplexity\n",
    "\n",
    "perplexity = lda1.log_perplexity(bow_dataset) # -8.09\n",
    "# lda2 = -8.13357"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

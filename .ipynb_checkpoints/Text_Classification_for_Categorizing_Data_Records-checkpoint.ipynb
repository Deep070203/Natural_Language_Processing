{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f86833c6",
   "metadata": {},
   "source": [
    "# Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f13ca9",
   "metadata": {},
   "source": [
    "The process of categorizing text into organized categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7233e93c",
   "metadata": {},
   "source": [
    "1. Data Collection: From a website or a database\n",
    "2. Preprocessing: To Remove anything that is not goint to be needed in order to understand the context and meaning.\n",
    "3. Feature Extraction: The key features that are going to be useful in determining what do we mean by that text and how can we classify into multiple categories.\n",
    "4. Model training: Pick a classification model that will enable us to pre-label the data and explain us the categories that the dataset belongs to.\n",
    "5. Prediction: Which class does our text belongs to?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19508487",
   "metadata": {},
   "source": [
    "## Component of Text Classification Sytems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea11a659",
   "metadata": {},
   "source": [
    "Data Source: Documents, Online Articles, Collection using Web Scraping and APIs\n",
    "\n",
    "Preprocessing tools and libraries: Cleaning -> Tokenize -> Normalize -> Stop Words Removal -> Stemming and Lemmatizing\n",
    "\n",
    "Feature extraction: Vectorization (Transforming into Numerical Values), Embeddings (Capturing Semantic Meaning)\n",
    "\n",
    "Classification Algorithms: Naive Bayes, Logistic Regression, Support Vector Machine, Decision Trees, Random Forest, Neural Networks\n",
    "\n",
    "Evaluation and Optimization (Using accuracy optimization): Metrics, Hyperparameter tuning (Adjusting model parameters), Cross Validation (Testing Using Subsets of the data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9908b0c6",
   "metadata": {},
   "source": [
    "## Binary vs. Multi-class Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ba508e",
   "metadata": {},
   "source": [
    "Binary Classification: Categorizing data into two distinct groups\n",
    "\n",
    "Examples: Email Filtering, Sentiment Analysis\n",
    "\n",
    "Characteristics: Clear-cut decision boundary, Simpler as it involves only two classes, Commonly used for yes-no type decisions\n",
    "\n",
    "Mutli-Class Classification: More than two groups\n",
    "\n",
    "Examples: News Categorization, Product Categorization\n",
    "\n",
    "Characteristics: Multiple decision boundaries, More complex due to presence of several classes, Used when data can belong to multiple distinct categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83d40b1",
   "metadata": {},
   "source": [
    "### Feature Selection Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c395487",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2 # select some of the best features in our text\n",
    "\n",
    "texts = [\"Sport news\", \"Cooking blog\"]\n",
    "\n",
    "labels = [0, 1] # 0 for sports, 1 for cooking\n",
    "\n",
    "X = TfidfVectorizer().fit_transform(texts) # Converting text data into numerical values\n",
    "\n",
    "s = SelectKBest(chi2, k=2).fit(X, labels) # Select the top features which are relevant\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7dc4ea",
   "metadata": {},
   "source": [
    "## Text Preprocessing and Vectorization Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35f5848",
   "metadata": {},
   "source": [
    "Vectorization methods = Bag-of-Words, TF-IDF, Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b85d1aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5 0.5 0.5 0.5]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "texts = [\"Machine Learning is fascinating\"]\n",
    "\n",
    "# Initialize and apply TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "\n",
    "print(tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e6384b",
   "metadata": {},
   "source": [
    "## Preprocessing the Profiles Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d52a9d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/deepshah/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/deepshah/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Setup packages\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder, normalize\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3b4aa5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>company</th>\n",
       "      <th>position</th>\n",
       "      <th>industry</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>John</td>\n",
       "      <td>Doe</td>\n",
       "      <td>ABC Corp</td>\n",
       "      <td>Marketing Manager</td>\n",
       "      <td>Technology</td>\n",
       "      <td>San Francisco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jane</td>\n",
       "      <td>Smith</td>\n",
       "      <td>XYZ Inc</td>\n",
       "      <td>Social Media Specialist</td>\n",
       "      <td>Advertising &amp; Marketing</td>\n",
       "      <td>New York</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Michael</td>\n",
       "      <td>Johnson</td>\n",
       "      <td>123 Company</td>\n",
       "      <td>Digital Marketing Analyst</td>\n",
       "      <td>Consulting</td>\n",
       "      <td>Chicago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sarah</td>\n",
       "      <td>Williams</td>\n",
       "      <td>ABC Corp</td>\n",
       "      <td>Content Writer</td>\n",
       "      <td>Media &amp; Publishing</td>\n",
       "      <td>London</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>David</td>\n",
       "      <td>Brown</td>\n",
       "      <td>XYZ Inc</td>\n",
       "      <td>Brand Manager</td>\n",
       "      <td>Consumer Goods</td>\n",
       "      <td>Miami</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  first_name last_name      company                   position  \\\n",
       "0       John       Doe     ABC Corp          Marketing Manager   \n",
       "1       Jane     Smith      XYZ Inc    Social Media Specialist   \n",
       "2    Michael   Johnson  123 Company  Digital Marketing Analyst   \n",
       "3      Sarah  Williams     ABC Corp             Content Writer   \n",
       "4      David     Brown      XYZ Inc              Brand Manager   \n",
       "\n",
       "                  industry       location  \n",
       "0               Technology  San Francisco  \n",
       "1  Advertising & Marketing       New York  \n",
       "2               Consulting        Chicago  \n",
       "3       Media & Publishing         London  \n",
       "4           Consumer Goods          Miami  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Dataset\n",
    "df = pd.read_csv('Demo Profiles.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c00c6e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Preprocessing Techniques\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = re.sub(f'[^\\w\\s]','',text)\n",
    "    text = re.sub(f'\\d+','',text)\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Remove Stop Words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed = [stemmer.stem(word) for word in tokens]\n",
    "    return ' '.join(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36ac233b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>company</th>\n",
       "      <th>position</th>\n",
       "      <th>industry</th>\n",
       "      <th>location</th>\n",
       "      <th>processed_positon</th>\n",
       "      <th>processed_position</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>John</td>\n",
       "      <td>Doe</td>\n",
       "      <td>ABC Corp</td>\n",
       "      <td>Marketing Manager</td>\n",
       "      <td>Technology</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>market manag</td>\n",
       "      <td>market manag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jane</td>\n",
       "      <td>Smith</td>\n",
       "      <td>XYZ Inc</td>\n",
       "      <td>Social Media Specialist</td>\n",
       "      <td>Advertising &amp; Marketing</td>\n",
       "      <td>New York</td>\n",
       "      <td>social media specialist</td>\n",
       "      <td>social media specialist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Michael</td>\n",
       "      <td>Johnson</td>\n",
       "      <td>123 Company</td>\n",
       "      <td>Digital Marketing Analyst</td>\n",
       "      <td>Consulting</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>digit market analyst</td>\n",
       "      <td>digit market analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sarah</td>\n",
       "      <td>Williams</td>\n",
       "      <td>ABC Corp</td>\n",
       "      <td>Content Writer</td>\n",
       "      <td>Media &amp; Publishing</td>\n",
       "      <td>London</td>\n",
       "      <td>content writer</td>\n",
       "      <td>content writer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>David</td>\n",
       "      <td>Brown</td>\n",
       "      <td>XYZ Inc</td>\n",
       "      <td>Brand Manager</td>\n",
       "      <td>Consumer Goods</td>\n",
       "      <td>Miami</td>\n",
       "      <td>brand manag</td>\n",
       "      <td>brand manag</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  first_name last_name      company                   position  \\\n",
       "0       John       Doe     ABC Corp          Marketing Manager   \n",
       "1       Jane     Smith      XYZ Inc    Social Media Specialist   \n",
       "2    Michael   Johnson  123 Company  Digital Marketing Analyst   \n",
       "3      Sarah  Williams     ABC Corp             Content Writer   \n",
       "4      David     Brown      XYZ Inc              Brand Manager   \n",
       "\n",
       "                  industry       location        processed_positon  \\\n",
       "0               Technology  San Francisco             market manag   \n",
       "1  Advertising & Marketing       New York  social media specialist   \n",
       "2               Consulting        Chicago     digit market analyst   \n",
       "3       Media & Publishing         London           content writer   \n",
       "4           Consumer Goods          Miami              brand manag   \n",
       "\n",
       "        processed_position  \n",
       "0             market manag  \n",
       "1  social media specialist  \n",
       "2     digit market analyst  \n",
       "3           content writer  \n",
       "4              brand manag  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the technique\n",
    "df['processed_position'] = df['position'].apply(preprocess_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74b00293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 1 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 1 0 0]]\n",
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.60225663 0.         0.        ]\n",
      " [0.         0.         0.62026425 ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.86288949 0.        ]\n",
      " [0.         0.         0.         ... 0.60225663 0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Process of Text Vectorization\n",
    "\n",
    "# A: Bag-of-Words\n",
    "vectorizer = CountVectorizer()\n",
    "bow_matrix = vectorizer.fit_transform(df['processed_position'])\n",
    "print(bow_matrix.toarray())\n",
    "\n",
    "# B: TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X = tfidf_vectorizer.fit_transform(df['processed_position'])\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "418db6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.60225663 0.         0.        ]\n",
      " [0.         0.         0.62026425 ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.86288949 0.        ]\n",
      " [0.         0.         0.         ... 0.60225663 0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Normalize the Vectorized data\n",
    "normalized_matrix = normalize(X, norm='l2', axis=1)\n",
    "print(normalized_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "10586ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. Technology\n",
      "1. Advertising & Marketing\n",
      "2. Consulting\n",
      "3. Media & Publishing\n",
      "4. Consumer Goods\n",
      "5. E-commerce\n",
      "6. Fashion & Apparel\n",
      "7. Beauty & Cosmetics\n",
      "8. Market Research\n",
      "9.  Marketing Coordinator\n"
     ]
    }
   ],
   "source": [
    "# Encode the Target Variable\n",
    "unique_values = df['industry'].unique()\n",
    "for i, value in enumerate(unique_values, 0):\n",
    "    print(f\"{i}. {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "19b9756c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 1 3 8 4 5 6 9 1 3 8 4 2 7 9 1 8 5 6 9 3 8 4 1 5 6 9 1 3 8 4 9 5 6 8 4 2\n",
      " 7 9 1 8 5 6 9 3 8 4 1 5 6 9 1 3 8 4 9 5 6 8 4 2 7 9 1 8 5 6 9 3 8 4 1 5 6\n",
      " 9 1 3 8 4 9 5 6 8 4 2 7 9 1 0 5 6 9 3 8 4 1 5 6 9 1]\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df['industry'])\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef95215",
   "metadata": {},
   "source": [
    "# Modeling, Evaluation and Advanced Classification Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d2138a",
   "metadata": {},
   "source": [
    "## Exploration of Classification Models Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cceb19",
   "metadata": {},
   "source": [
    "1. Naive Bayes: The likelihood of a data point belonging to a particular class. Best for Spam detection, sentiment analysis, document classification. It is simple, fast and effective with large feature sets.\n",
    "\n",
    "2. Logistic Regression: It calculates the likelihood of a specific input belonging to a particular category. Best for Binary Classification, Email filtering, Fraud detection. It outputs probabilities, Good interpretability, Efficient\n",
    "\n",
    "3. Support Vector Machine: The process of drawing a line between two groups of points to separate them as clearly as possible. Best for Categorizing emails, articles, web pages. Effectiveness in high dimensions, Adaptable to various data types, Tends to be very accurate.\n",
    "\n",
    "4. Decision Trees: Tree-like structure with branches representing decision paths and leaves representing outcomes or decisions. Best for strategic decision, classification tasks. Visualize the decision-making process, Doesn't require normalization.\n",
    "\n",
    "5. Random Forest: A set of decision trees are used to predict classes. The class with the highest number of votes is chosen as the prediction. Best for Scenarios with multiple classes, Understanding feature importance. High accuracy and robustness, good performance for datasets with unbalanced classes.\n",
    "\n",
    "6. Neural Networks: Made up of layers capable of detecting patterns in data, ranging from simple to complex. Best for Language translation, Sentiment analysis and text generation. Learning features directly from data, adapt to wide range of tasks, can model complex features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e097638f",
   "metadata": {},
   "source": [
    "Hyperparameter tuning: The process of finding the optimal combination of parameters that yields best model performance.\n",
    "\n",
    "Methods:\n",
    "1. Grid Search\n",
    "2. Random Search\n",
    "3. Automated Machine Learning Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e638ff",
   "metadata": {},
   "source": [
    "Cross-validation: Give models subsets of data, Each subset will be tested in different way, evaluate the model's learning progress accurately.\n",
    "\n",
    "Text Dataset -> Split data into folds -> Train some folds -> Test the performance using the other folds -> Switch the folds now for training and testing -> Calculate the avg performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c9affc45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/deepshah/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "# Sentiment Analysis Example\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "sa = SentimentIntensityAnalyzer()\n",
    "\n",
    "text = \"I love this phone. The camera is amazing!\"\n",
    "\n",
    "sentiment = \"Positive\" if sa.polarity_scores(text)['compound'] >= 0 else \"Negative\"\n",
    "\n",
    "print(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ccd4d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam\n"
     ]
    }
   ],
   "source": [
    "# Spam Detection Example\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "texts = [\"Win a free smartphone!\", \"Lunch at noon\"]\n",
    "labels = [1,0] # 1 for spam and 0 for not spam\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(texts)\n",
    "model = MultinomialNB().fit(X, labels)\n",
    "\n",
    "new_text = vectorizer.transform([\"Free Money!!!\"])\n",
    "predicted = model.predict(new_text)\n",
    "\n",
    "print(\"Spam\" if predicted[0]==1 else \"Not Spam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5bedea7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended:  Action movie\n"
     ]
    }
   ],
   "source": [
    "# Content Recommendation Example\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "contents = [\"Action movie\", \"Romantic movie\", \"Documentary about nature\"]\n",
    "\n",
    "user_favorite = \"Action and adventure\"\n",
    "\n",
    "tfidf = TfidfVectorizer().fit_transform(contents+[user_favorite]) # Transforming text into TF-IDF vectors\n",
    "\n",
    "sm = linear_kernel(tfidf[-1], tfidf[:-1]).flatten() # Calculating the similarity of the user fav content with the list\n",
    "\n",
    "print(\"Recommended: \", contents[sm.argmax()]) # Printing most similar content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e8f58c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved:  AI trendsParis travel\n"
     ]
    }
   ],
   "source": [
    "# Information Retrieval Example\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "documents = [\"financial markets\", \"AI trends\" \"Paris travel\"]\n",
    "\n",
    "query = \"AI technology\"\n",
    "\n",
    "vec = TfidfVectorizer().fit_transform(documents + [query])\n",
    "# doc_vec = vec.transform(documents)\n",
    "# query_vec = vec.transform([query])\n",
    "\n",
    "sm = linear_kernel(vec[-1], vec[:-1]).flatten()\n",
    "\n",
    "print(\"Retrieved: \", documents[sm.argmax()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bceeef",
   "metadata": {},
   "source": [
    "## Classifying Professional profiles Into Multiple Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "162b6ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/deepshah/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/deepshah/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing the Profiles Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8bcaa3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Demo Profiles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2a8aa385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]','', text)\n",
    "    text = re.sub(r'\\d+','', text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed = [stemmer.stem(word) for word in tokens]\n",
    "    return ' '.join(stemmed)\n",
    "\n",
    "df['processed_text'] = df['position'].apply(preprocess_text)\n",
    "\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "X = tfidf_vec.fit_transform(df['processed_text'])\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df['industry'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a2bcab02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3d436bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Models\n",
    "models = {\n",
    "    \"Naive Bayes\": MultinomialNB(),\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"SVM\": SVC(),\n",
    "    \"Random Forest\": RandomForestClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "58cb71b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Hyperparameter Tuning\n",
    "param_grid = {\n",
    "    \"Naive Bayes\": {'alpha': [0.1, 1, 10]},\n",
    "    \"Logistic Regression\": {'C': [0.1, 1, 10]},\n",
    "    \"SVM\": {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']},\n",
    "    \"Random Forest\": {'n_estimators': [10, 50, 100], 'max_depth': [None, 10, 20]}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bdd503d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes: Best Params: {'alpha': 0.1}, Cross-Val Score: 0.7875\n",
      "Logistic Regression: Best Params: {'C': 10}, Cross-Val Score: 0.85\n",
      "SVM: Best Params: {'C': 10, 'kernel': 'linear'}, Cross-Val Score: 0.8750000000000001\n",
      "Random Forest: Best Params: {'max_depth': None, 'n_estimators': 100}, Cross-Val Score: 0.8250000000000001\n"
     ]
    }
   ],
   "source": [
    "# Train and Evaluate Models\n",
    "best_models = {}\n",
    "model_scores = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    grid = GridSearchCV(model, param_grid[name], cv=4)\n",
    "    grid.fit(X_train, y_train)\n",
    "    best_model = grid.best_estimator_\n",
    "    best_models[name] = best_model\n",
    "    scores = cross_val_score(best_model, X_train, y_train, cv=4)\n",
    "    avg_score = np.mean(scores)\n",
    "    model_scores[name] = avg_score\n",
    "    print(f\"{name}: Best Params: {grid.best_params_}, Cross-Val Score: {avg_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1a577f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00         3\n",
      "           2       1.00      1.00      1.00         1\n",
      "           3       0.33      1.00      0.50         1\n",
      "           4       1.00      1.00      1.00         3\n",
      "           5       1.00      1.00      1.00         3\n",
      "           6       1.00      0.33      0.50         3\n",
      "           7       1.00      1.00      1.00         1\n",
      "           8       1.00      1.00      1.00         1\n",
      "           9       1.00      1.00      1.00         4\n",
      "\n",
      "    accuracy                           0.90        20\n",
      "   macro avg       0.93      0.93      0.89        20\n",
      "weighted avg       0.97      0.90      0.90        20\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = best_models[max(model_scores, key=model_scores.get)].predict(X_test)\n",
    "print(f\"Test Set Report:\\n{classification_report(y_test, y_pred)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "75f58c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted industry for Digital Marketing Specialist is: E-commerce\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "digital_ex = \"Digital Marketing Specialist\"\n",
    "\n",
    "processed_ex = preprocess_text(digital_ex)\n",
    "\n",
    "vec_ex = tfidf_vec.transform([processed_ex])\n",
    "\n",
    "predicted_cat_idx = best_models[\"SVM\"].predict(vec_ex)\n",
    "pred_cat = label_encoder.inverse_transform(predicted_cat_idx)\n",
    "\n",
    "print(f\"The predicted industry for {digital_ex} is: {pred_cat[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
